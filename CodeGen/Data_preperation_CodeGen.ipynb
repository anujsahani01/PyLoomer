{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install -q -U git+https://github.com/huggingface/peft.git\n!pip install -q -U git+https://github.com/huggingface/accelerate.git\n# ! pip install datasets\n! pip install torch","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:51:52.666553Z","iopub.execute_input":"2023-09-14T15:51:52.667245Z","iopub.status.idle":"2023-09-14T15:54:06.104531Z","shell.execute_reply.started":"2023-09-14T15:51:52.667207Z","shell.execute_reply":"2023-09-14T15:54:06.103200Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"! pip install fasttext","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:54:06.107955Z","iopub.execute_input":"2023-09-14T15:54:06.108399Z","iopub.status.idle":"2023-09-14T15:54:18.080474Z","shell.execute_reply.started":"2023-09-14T15:54:06.108355Z","shell.execute_reply":"2023-09-14T15:54:18.079225Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: fasttext in /opt/conda/lib/python3.10/site-packages (0.9.2)\nRequirement already satisfied: pybind11>=2.2 in /opt/conda/lib/python3.10/site-packages (from fasttext) (2.11.1)\nRequirement already satisfied: setuptools>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from fasttext) (68.0.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fasttext) (1.23.5)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Loading the model","metadata":{}},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# special tokens for prompting\nsystem_token = \"<SYSTEM_TASK:>\"\nuser_token = \"<USER_TASK:>\"\nassistant_token = \"<ASSISTANT_TASK:>\"\nend_token = \"<END_TASK>\"\n\n\nmodel_checkpoint = 'Salesforce/codegen-350M-mono'\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint,\n                                          additional_special_tokens = [\"<SYSTEM_TASK:>\", \"<USER_TASK:>\", \"<ASSISTANT_TASK:>\", \"<END_TASK>\"],\n                                          pad_token = \"<PAD>\",\n                                          )","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:54:18.082552Z","iopub.execute_input":"2023-09-14T15:54:18.082936Z","iopub.status.idle":"2023-09-14T15:54:21.160743Z","shell.execute_reply.started":"2023-09-14T15:54:18.082901Z","shell.execute_reply":"2023-09-14T15:54:21.159661Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/240 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cba1f38278884bdbb0ab9ca9c80695e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8faccc136cef4c838a69f734322b6efd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29393dc520774b9eb48731af5724f6d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48f41b6c236d4c0e822bf6ea4a71794f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)in/added_tokens.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8b9783befa848339473a9d5e1de9ce3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02b59f0e7afa440bb098eea4f271da19"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:54:21.163436Z","iopub.execute_input":"2023-09-14T15:54:21.164397Z","iopub.status.idle":"2023-09-14T15:54:21.175272Z","shell.execute_reply.started":"2023-09-14T15:54:21.164368Z","shell.execute_reply":"2023-09-14T15:54:21.173992Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"CodeGenTokenizerFast(name_or_path='Salesforce/codegen-350M-mono', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<PAD>', 'additional_special_tokens': ['<SYSTEM_TASK:>', '<USER_TASK:>', '<ASSISTANT_TASK:>', '<END_TASK>']}, clean_up_tokenization_spaces=True)"},"metadata":{}}]},{"cell_type":"code","source":"special_token_dict = tokenizer.special_tokens_map\nprint(special_token_dict)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:54:21.176951Z","iopub.execute_input":"2023-09-14T15:54:21.177418Z","iopub.status.idle":"2023-09-14T15:54:21.189040Z","shell.execute_reply.started":"2023-09-14T15:54:21.177384Z","shell.execute_reply":"2023-09-14T15:54:21.187874Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"{'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<PAD>', 'additional_special_tokens': ['<SYSTEM_TASK:>', '<USER_TASK:>', '<ASSISTANT_TASK:>', '<END_TASK>']}\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.add_special_tokens(special_token_dict)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:54:21.190552Z","iopub.execute_input":"2023-09-14T15:54:21.191021Z","iopub.status.idle":"2023-09-14T15:54:21.203825Z","shell.execute_reply.started":"2023-09-14T15:54:21.190989Z","shell.execute_reply":"2023-09-14T15:54:21.202855Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"context_length = 3000","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:54:21.207160Z","iopub.execute_input":"2023-09-14T15:54:21.207438Z","iopub.status.idle":"2023-09-14T15:54:21.214116Z","shell.execute_reply.started":"2023-09-14T15:54:21.207416Z","shell.execute_reply":"2023-09-14T15:54:21.212933Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom transformers import BitsAndBytesConfig, AutoConfig, AutoModelForCausalLM, AutoModelWithLMHead\nimport torch\nfrom accelerate import init_empty_weights, infer_auto_device_map","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:54:21.216335Z","iopub.execute_input":"2023-09-14T15:54:21.217007Z","iopub.status.idle":"2023-09-14T15:54:24.799825Z","shell.execute_reply.started":"2023-09-14T15:54:21.216947Z","shell.execute_reply":"2023-09-14T15:54:24.798817Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model_checkpoint = 'Salesforce/codegen-350M-mono'\n\nconfig = AutoConfig.from_pretrained(model_checkpoint,\n                                    vocab_size=tokenizer.vocab_size,\n                                    bos_token_id=tokenizer.bos_token_id,\n                                    eos_token_id=tokenizer.eos_token_id,\n)\n\nwith init_empty_weights():\n    model = AutoModelForCausalLM.from_config(config)\n\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:54:24.801460Z","iopub.execute_input":"2023-09-14T15:54:24.802002Z","iopub.status.idle":"2023-09-14T15:54:25.588899Z","shell.execute_reply.started":"2023-09-14T15:54:24.801969Z","shell.execute_reply":"2023-09-14T15:54:25.587858Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/999 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eee84f4c5f6d416f963c579f64d670b3"}},"metadata":{}},{"name":"stdout","text":"CodeGenForCausalLM(\n  (transformer): CodeGenModel(\n    (wte): Embedding(50257, 1024)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-19): 20 x CodeGenBlock(\n        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (attn): CodeGenAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (qkv_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n        )\n        (mlp): CodeGenMLP(\n          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=50257, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"device_map = infer_auto_device_map(model, no_split_module_classes = ['CodeGenBlock'])\nprint(device_map)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:54:25.592736Z","iopub.execute_input":"2023-09-14T15:54:25.593042Z","iopub.status.idle":"2023-09-14T15:54:30.749057Z","shell.execute_reply.started":"2023-09-14T15:54:25.593015Z","shell.execute_reply":"2023-09-14T15:54:30.748048Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"{'': 0}\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\nnf4_config = BitsAndBytesConfig(\n   load_in_2bit=True,\n   bnb_4bit_quant_type=\"nf4\",\n   bnb_4bit_use_double_quant=True,\n   bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n\nmodel_id = 'Salesforce/codegen-350M-mono'\nmodel =AutoModelForCausalLM.from_pretrained(model_id,\n                                            config = config,\n                                            device_map=\"auto\",\n                                            quantization_config=nf4_config,\n                                            torch_dtype=torch.bfloat16)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:54:30.750516Z","iopub.execute_input":"2023-09-14T15:54:30.751516Z","iopub.status.idle":"2023-09-14T15:54:37.687784Z","shell.execute_reply.started":"2023-09-14T15:54:30.751482Z","shell.execute_reply":"2023-09-14T15:54:37.686683Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/797M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee778c9542334f74b56a1f97a0b7bbfd"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(model))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:54:37.689529Z","iopub.execute_input":"2023-09-14T15:54:37.689974Z","iopub.status.idle":"2023-09-14T15:54:37.699400Z","shell.execute_reply.started":"2023-09-14T15:54:37.689939Z","shell.execute_reply":"2023-09-14T15:54:37.698467Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"trainable model parameters: 356712448\nall model parameters: 356712448\npercentage of trainable model parameters: 100.00%\n","output_type":"stream"}]},{"cell_type":"code","source":"model.resize_token_embeddings(len(tokenizer))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:54:37.700894Z","iopub.execute_input":"2023-09-14T15:54:37.702050Z","iopub.status.idle":"2023-09-14T15:54:37.725864Z","shell.execute_reply.started":"2023-09-14T15:54:37.702017Z","shell.execute_reply":"2023-09-14T15:54:37.724854Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 50300. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Embedding(50300, 1024)"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nfrom transformers import BitsAndBytesConfig, AutoConfig, AutoModelForCausalLM, AutoModelWithLMHead\nfrom transformers import GenerationConfig\nimport torch\nfrom accelerate import init_empty_weights, infer_auto_device_map\nimport re\nfrom functools import reduce","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:54:37.727211Z","iopub.execute_input":"2023-09-14T15:54:37.727926Z","iopub.status.idle":"2023-09-14T15:54:37.733190Z","shell.execute_reply.started":"2023-09-14T15:54:37.727864Z","shell.execute_reply":"2023-09-14T15:54:37.732209Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"**TRYING DIFFERENT RPOMPTS**","metadata":{}},{"cell_type":"code","source":"add_prompt = '<SYSTEM_TASK:>\\nGiven the following code description, write Python code to implement the functionality described below\\n<END_TASK>\\n<USER_TASK:>\\nDescription:\\n'\n\ntext_prompt = 'def python_task1():\"\"\"Initialize VGG16 model a deep learning model trained on imagenet for performing Image Classification in the VGG16_model_architecture function\"\"\"def python_task2():\"\"\"Split the data X, Y in to train and test data using sklearn\"\"\"def python_task3():\"\"\"Make prediction using the deep learning model defined above in VGG16_model_architecture function\"\"\"'\n\nfinal_prompt = add_prompt + text_prompt\nprint(final_prompt)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:18:02.848820Z","iopub.execute_input":"2023-09-12T07:18:02.849809Z","iopub.status.idle":"2023-09-12T07:18:02.860041Z","shell.execute_reply.started":"2023-09-12T07:18:02.849772Z","shell.execute_reply":"2023-09-12T07:18:02.858727Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"<SYSTEM_TASK:>\nGiven the following code description, write Python code to implement the functionality described below\n<END_TASK>\n<USER_TASK:>\nDescription:\ndef python_task1():\"\"\"Initialize VGG16 model a deep learning model trained on imagenet for performing Image Classification in the VGG16_model_architecture function\"\"\"def python_task2():\"\"\"Split the data X, Y in to train and test data using sklearn\"\"\"def python_task3():\"\"\"Make prediction using the deep learning model defined above in VGG16_model_architecture function\"\"\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This prompt is working for now","metadata":{}},{"cell_type":"code","source":"add_prompt = '<SYSTEM_TASK:>\\nGiven the following code description, write Python code to implement the functionality described below\\n<END_TASK>\\n<USER_TASK:>\\nDescription:\\n'\ntext_prompt = 'def VGG16_model_architecture():\"\"\"Initialize VGG16 model a deep learning model trained on imagenet for performing Image Classification in the VGG16_model_architecture function\"\"\"def train_test_split():\"\"\"Split the data X, Y in to train and test data using sklearn\"\"\"def model_predict():\"\"\"Make prediction using the deep learning model defined above in VGG16_model_architecture function\"\"\"'\nfinal_prompt = add_prompt + text_prompt\n\n\ngeneration_config = GenerationConfig(max_new_tokens=500, temperature=0.6, do_sample = True, top_p = 3)\ninputs = tokenizer(final_prompt, return_tensors = 'pt')\n\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        generation_config = generation_config,\n        max_new_tokens = 500,\n        pad_token_id = tokenizer.pad_token_id,\n    )[0],\n    skip_special_tokens=True\n)\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{final_prompt}')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:30:11.498267Z","iopub.execute_input":"2023-09-12T07:30:11.498665Z","iopub.status.idle":"2023-09-12T07:30:37.279969Z","shell.execute_reply.started":"2023-09-12T07:30:11.498635Z","shell.execute_reply":"2023-09-12T07:30:37.278921Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n<SYSTEM_TASK:>\nGiven the following code description, write Python code to implement the functionality described below\n<END_TASK>\n<USER_TASK:>\nDescription:\ndef VGG16_model_architecture():\"\"\"Initialize VGG16 model a deep learning model trained on imagenet for performing Image Classification in the VGG16_model_architecture function\"\"\"def train_test_split():\"\"\"Split the data X, Y in to train and test data using sklearn\"\"\"def model_predict():\"\"\"Make prediction using the deep learning model defined above in VGG16_model_architecture function\"\"\"\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n\nGiven the following code description, write Python code to implement the functionality described below\n\n\nDescription:\ndef VGG16_model_architecture():\"\"\"Initialize VGG16 model a deep learning model trained on imagenet for performing Image Classification in the VGG16_model_architecture function\"\"\"def train_test_split():\"\"\"Split the data X, Y in to train and test data using sklearn\"\"\"def model_predict():\"\"\"Make prediction using the deep learning model defined above in VGG16_model_architecture function\"\"\"\n\n# def VGG16_model_architecture():\"\"\"Initialize VGG16 model a deep learning model trained on imagenet for performing Image Classification in the VGG16_model_architecture function\"\"\"\n\n# def train_test_split():\"\"\"Split the data X, Y in to train and test data using sklearn\"\"\"\n# def model_predict():\"\"\"Make prediction using the deep learning model defined above in VGG16_model_architecture function\"\"\"\n\ndef VGG16_model_architecture():\n    \"\"\"Initialize VGG16 model a deep learning model trained on imagenet for performing Image Classification in the VGG16_model_architecture function\"\"\"\n    # define the VGG16 network\n    # The input is the input layer\n    # The output is the \"flatten\" layer\n    # The output has the output layer of 3 fully-connected nodes\n    # The output has the output layer of 2 fully-connected nodes\n    # The output has the output layer of 1 fully-connected node\n    \n    model = Sequential()\n    \n    model.add(Conv2D(64, (3, 3), padding='same', input_shape=(32, 32, 3), activation='relu'))\n    # 32, 32, 3\n    model.add(BatchNormalization())\n    # 32, 32, 3\n    model.add(Dropout(0.5))\n    # 32,mini,3,3\n    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n    # 32, 32, 3\n    model.add(BatchNormalization())\n    # 32, 32, 3\n    model.add(Dropout(0.5))\n    # 32,mini,3,3\n    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n    # 32, 32, 3\n    model.add(BatchNormalization())\n    # 32, 32, 3\n    model.add(Dropout(0.5))\n    # 32, mini,3,3\n    model.add(Conv2D(64, (3, 3),\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This is also working (try in on orignal dataset)","metadata":{}},{"cell_type":"code","source":"add_prompt = '<SYSTEM_TASK:>\\nGiven the following code description, write Python code to implement the functionality described below\\n<END_TASK>\\n<USER_TASK:>\\nDescription:\\n'\ntext_prompt = 'def python_task_1():\\n\"\"\"\\nInitialize VGG16 model a deep learning model trained on imagenet for performing Image Classification in the VGG16_model_architecture function\\n\"\"\"\\ndef python_task_2():\\n\"\"\"\\nSplit the data X, Y in to train and test data using sklearn\\n\"\"\"\\ndef python_task_3():\\n\"\"\"\\nMake prediction using the deep learning model defined above in VGG16_model_architecture function\\n\"\"\"\\n'\nfinal_prompt = add_prompt + text_prompt\nprint(final_prompt)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:18:32.271151Z","iopub.execute_input":"2023-09-12T07:18:32.272839Z","iopub.status.idle":"2023-09-12T07:18:32.280867Z","shell.execute_reply.started":"2023-09-12T07:18:32.272776Z","shell.execute_reply":"2023-09-12T07:18:32.279779Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"<SYSTEM_TASK:>\nGiven the following code description, write Python code to implement the functionality described below\n<END_TASK>\n<USER_TASK:>\nDescription:\ndef python_task_1():\n\"\"\"\nInitialize VGG16 model a deep learning model trained on imagenet for performing Image Classification in the VGG16_model_architecture function\n\"\"\"\ndef python_task_2():\n\"\"\"\nSplit the data X, Y in to train and test data using sklearn\n\"\"\"\ndef python_task_3():\n\"\"\"\nMake prediction using the deep learning model defined above in VGG16_model_architecture function\n\"\"\"\n\n","output_type":"stream"}]},{"cell_type":"code","source":"add_prompt = '<SYSTEM_TASK:>\\nGiven the following code description, write Python code to implement the functionality described below\\n<END_TASK>\\n<USER_TASK:>\\nDescription:\\n'\ntext_prompt = 'def python_task_1():\\n\"\"\"\\nInitialize VGG16 model a deep learning model trained on imagenet for performing Image Classification in the VGG16_model_architecture function\\n\"\"\"\\ndef python_task_2():\\n\"\"\"\\nSplit the data X, Y in to train and test data using sklearn\\n\"\"\"\\ndef python_task_3():\\n\"\"\"\\nMake prediction using the deep learning model defined above in VGG16_model_architecture function\\n\"\"\"\\n'\nfinal_prompt = add_prompt + text_prompt\n\ngeneration_config = GenerationConfig(max_new_tokens=500, temperature=0.9, do_sample = True, top_p = 3)\ninputs = tokenizer(final_prompt, return_tensors = 'pt')\n\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        generation_config = generation_config,\n        max_new_tokens = 500,\n        pad_token_id = tokenizer.pad_token_id,\n    )[0],\n    skip_special_tokens=True\n)\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{final_prompt}')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:21:57.701883Z","iopub.execute_input":"2023-09-12T07:21:57.702859Z","iopub.status.idle":"2023-09-12T07:22:22.812532Z","shell.execute_reply.started":"2023-09-12T07:21:57.702820Z","shell.execute_reply":"2023-09-12T07:22:22.811487Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n<SYSTEM_TASK:>\nGiven the following code description, write Python code to implement the functionality described below\n<END_TASK>\n<USER_TASK:>\nDescription:\ndef python_task_1():\n\"\"\"\nInitialize VGG16 model a deep learning model trained on imagenet for performing Image Classification in the VGG16_model_architecture function\n\"\"\"\ndef python_task_2():\n\"\"\"\nSplit the data X, Y in to train and test data using sklearn\n\"\"\"\ndef python_task_3():\n\"\"\"\nMake prediction using the deep learning model defined above in VGG16_model_architecture function\n\"\"\"\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n\nGiven the following code description, write Python code to implement the functionality described below\n\n\nDescription:\ndef python_task_1():\n\"\"\"\nInitialize VGG16 model a deep learning model trained on imagenet for performing Image Classification in the VGG16_model_architecture function\n\"\"\"\ndef python_task_2():\n\"\"\"\nSplit the data X, Y in to train and test data using sklearn\n\"\"\"\ndef python_task_3():\n\"\"\"\nMake prediction using the deep learning model defined above in VGG16_model_architecture function\n\"\"\"\nfrom collections import namedtuple\nVGG_NAME = 'vgg16'\n\nVGG16 = VGG16_model_architecture(layers_included_by_type=('fc6', 'fc7', 'fc8', 'avgpool',),\n                                 l2_regularization_factor=0.5,\n                                 include_softmax=True)\n\n# Make the following imports line into the code used for this script. In the future we should use a different naming convention.\n# from keras.preprocessingMbps.image import ImageDataGenerator\n# from keras.preprocessing.image import img_to_array\nfrom math import trunc\nimport os \n# from keras.utils import to_categorical\nfrom keras.models import Model\n# from keras.applications.vgg16 import VGG16\nfrom keras.preprocessing import image\nimport numpy as np\n%load_ext autoreload\n%autoreload\n\nModelTxt = namedtuple('ModelTxt', liberalism_attrs)\n\n# from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\nfrom keras.callbacks import FileLogger  \nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom torch import nn\nfrom torch import optim\nimporttalex\n\n\nfrom keras.models import load_model  \nfrom keras import initializers\nfrom keras.optimizers import schedules\nimport copy\nfrom keras.layers import Conv2D\n\n##################################\n# Import the Csv Dataset \ncsv_path = r'data/test2_train_img.csv'\ncsv_train = np.loadtxt(csv_path, delimiter=\",\", dtype=\"object\", skiprows=1) \n\ncsv_train[:,-1] = csv_train[:,-1] - 1\ncolnames = [y for x,y in enumerate(csv_train[0,:])]\ntrain_dataset = csv.dictmin(csv_train[1:], colnames=colnames)\n# train and test of test image \n# train_\n","output_type":"stream"}]},{"cell_type":"code","source":"add_prompt = '<SYSTEM_TASK:>\\nGiven the following code description, write Python code to implement the functions described below line by line\\n<END_TASK>\\n<USER_TASK:>\\nDescription:\\n'\ntext_prompt = 'def Initialize_VGG16_model_a_deep_learning_model_trained_on_imagenet_for_performing_Image_Classification_in_the_VGG16_model_architecture_function():\\ndef Split_the_data_X,_Y_in_to_train_and_test_data_using_sklearn():\\ndef Make_prediction_using_the_deep_learning_model_defined_above_in_VGG16_model_architecture_function():'\nfinal_prompt = add_prompt + text_prompt\n\ngeneration_config = GenerationConfig(max_new_tokens=500, temperature=1.2, do_sample = True, top_p = 3)\ninputs = tokenizer(final_prompt, return_tensors = 'pt')\n\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        generation_config = generation_config,\n        max_new_tokens = 500,\n        pad_token_id = tokenizer.pad_token_id,\n    )[0],\n    skip_special_tokens=True\n)\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{final_prompt}')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:29:35.900764Z","iopub.execute_input":"2023-09-12T07:29:35.901165Z","iopub.status.idle":"2023-09-12T07:30:01.947247Z","shell.execute_reply.started":"2023-09-12T07:29:35.901134Z","shell.execute_reply":"2023-09-12T07:30:01.946134Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n<SYSTEM_TASK:>\nGiven the following code description, write Python code to implement the functions described below line by line\n<END_TASK>\n<USER_TASK:>\nDescription:\ndef Initialize_VGG16_model_a_deep_learning_model_trained_on_imagenet_for_performing_Image_Classification_in_the_VGG16_model_architecture_function():\ndef Split_the_data_X,_Y_in_to_train_and_test_data_using_sklearn():\ndef Make_prediction_using_the_deep_learning_model_defined_above_in_VGG16_model_architecture_function():\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n\nGiven the following code description, write Python code to implement the functions described below line by line\n\n\nDescription:\ndef Initialize_VGG16_model_a_deep_learning_model_trained_on_imagenet_for_performing_Image_Classification_in_the_VGG16_model_architecture_function():\ndef Split_the_data_X,_Y_in_to_train_and_test_data_using_sklearn():\ndef Make_prediction_using_the_deep_learning_model_defined_above_in_VGG16_model_architecture_function():\ndef Run_the_predicit_on_the_X_data_using_the_keras_included_in_the_deep_learning_model_defied_above_in_VGG16_model_architecture_: \ndef Image_Classification_vgg_19_deep_learning_model():\n# This one returns predictions\n# and the confusion matrics\ndef PredictImage_on_the_X_data_set():\n\"\"\"\nThis the prediction of the input image.\nThis function returns a class predicted that gives the number\nof positive and negative image(s).\n\"\"\"\n# This returns the predictions\n  Y_PRED_1 = vgg.predict(np.asarray([X/255, X]).transpose((0, 2, 3, 1)))\n  return Y_PRED_1,(np.sum(Y_PRED_1[0]), np.sum(Y_PRED_1[1]), np.sum(Y_PRED_1[2]))\"\"\"Note:The functions in the vgg_19 model need to be written in order\"\"\"\ndef predict_one_image_one_dof_the_model():\n\"\"\" This function returns a class predicted of the given input image\n\n\n\n This function must take in image input and returns the model predicted.\nIf no proper classifiers found (with predicted probabilities not set to zero), a return of 'NaN'\n\nAlso for Image classification models, this function assumes the images will\nbe in CIFAR-10 and it first reshapes them to 4 channels so the input could be used for an\nappropriate model.\n \"\"\"\n  IMG_SHAPE = (100,100,3)\n  print('This function is to perform image classification based on Deep Learning: \\n1) The model has 80 neurons per layer. \n    2) It consists of a dropout layer after each layer and an additional densenetlayer to reduce the number of neurons for each layer. \n    3) It has a fully_connected layer that is the output layer of all deep layer neurons \n      4) The fully connected layer does the soft signivation and is used to determine the corresponding classification. \n        5) The neural network for the fully neural network model that has 4 hidden layers\n","output_type":"stream"}]},{"cell_type":"code","source":"text_prompt = 'def VGG16_model_architecture():\"\"\"Initialize VGG16 model a deep learning model trained on imagenet for performing Image Classification in the VGG16_model_architecture function\"\"\"def train_test_split():\"\"\"Split the data X, Y in to train and test data using sklearn\"\"\"def model_predict():\"\"\"Make prediction using the deep learning model defined above in VGG16_model_architecture function\"\"\"'\nprint(text_prompt)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T07:41:20.282109Z","iopub.execute_input":"2023-09-12T07:41:20.282519Z","iopub.status.idle":"2023-09-12T07:41:20.289331Z","shell.execute_reply.started":"2023-09-12T07:41:20.282490Z","shell.execute_reply":"2023-09-12T07:41:20.288241Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"def VGG16_model_architecture():\"\"\"Initialize VGG16 model a deep learning model trained on imagenet for performing Image Classification in the VGG16_model_architecture function\"\"\"def train_test_split():\"\"\"Split the data X, Y in to train and test data using sklearn\"\"\"def model_predict():\"\"\"Make prediction using the deep learning model defined above in VGG16_model_architecture function\"\"\"\n","output_type":"stream"}]},{"cell_type":"code","source":"add_prompt = '<SYSTEM_TASK:>\\nSolve the following problem using Python, implementing the functions described below, one line at a time\\n<END_TASK>\\n<USER_TASK:>\\nDescription:\\n'\ntext_prompt = 'def python_task_1():\\n\"\"\" This function initialize VGG16 model a deep learning model trained on imagenet for performing Image Classification in the VGG16_model_architecture function \"\"\"\\ndef python_task_2():\\n\"\"\" This function Split the data X, Y in to train and test data using sklearn \"\"\"\\ndef python_task_3():\\n\"\"\" This function Make prediction using the deep learning model defined above in VGG16_model_architecture function \"\"\"\\n'\nfinal_prompt = add_prompt + text_prompt\nprint(final_prompt)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T10:13:21.929913Z","iopub.execute_input":"2023-09-12T10:13:21.930306Z","iopub.status.idle":"2023-09-12T10:13:21.936639Z","shell.execute_reply.started":"2023-09-12T10:13:21.930276Z","shell.execute_reply":"2023-09-12T10:13:21.935624Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"<SYSTEM_TASK:>\nSolve the following problem using Python, implementing the functions described below, one line at a time\n<END_TASK>\n<USER_TASK:>\nDescription:\ndef python_task_1():\n\"\"\" This function initialize VGG16 model a deep learning model trained on imagenet for performing Image Classification in the VGG16_model_architecture function \"\"\"\ndef python_task_2():\n\"\"\" This function Split the data X, Y in to train and test data using sklearn \"\"\"\ndef python_task_3():\n\"\"\" This function Make prediction using the deep learning model defined above in VGG16_model_architecture function \"\"\"\n\n","output_type":"stream"}]},{"cell_type":"code","source":"generation_config = GenerationConfig(max_new_tokens=500, temperature= 0.9, do_sample = True, top_p = 3)\ninputs = tokenizer(final_prompt, return_tensors = 'pt')\n\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        generation_config = generation_config,\n        max_new_tokens = 500,\n        pad_token_id = tokenizer.pad_token_id,\n    )[0],\n    skip_special_tokens=True\n)\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{final_prompt}')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"execution":{"iopub.status.busy":"2023-09-12T10:13:22.127205Z","iopub.execute_input":"2023-09-12T10:13:22.127897Z","iopub.status.idle":"2023-09-12T10:13:52.703113Z","shell.execute_reply.started":"2023-09-12T10:13:22.127861Z","shell.execute_reply":"2023-09-12T10:13:52.702114Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1535: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n<SYSTEM_TASK:>\nSolve the following problem using Python, implementing the functions described below, one line at a time\n<END_TASK>\n<USER_TASK:>\nDescription:\ndef python_task_1():\n\"\"\" This function initialize VGG16 model a deep learning model trained on imagenet for performing Image Classification in the VGG16_model_architecture function \"\"\"\ndef python_task_2():\n\"\"\" This function Split the data X, Y in to train and test data using sklearn \"\"\"\ndef python_task_3():\n\"\"\" This function Make prediction using the deep learning model defined above in VGG16_model_architecture function \"\"\"\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n\nSolve the following problem using Python, implementing the functions described below, one line at a time\n\n\nDescription:\ndef python_task_1():\n\"\"\" This function initialize VGG16 model a deep learning model trained on imagenet for performing Image Classification in the VGG16_model_architecture function \"\"\"\ndef python_task_2():\n\"\"\" This function Split the data X, Y in to train and test data using sklearn \"\"\"\ndef python_task_3():\n\"\"\" This function Make prediction using the deep learning model defined above in VGG16_model_architecture function \"\"\"\n\n# Python Task 4\n\n# Python Task 4: Implement VGG16_model_architecture function\n\n# Importing vgg16_model from keras\nfrom keras.models import Sequential\nfrom keras.layers import Convolution2D\nfrom keras.layers import Activation\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\n\n# Constructing a Sequential model\nvgg16 = Sequential()\n\n# Initializing VGG16 model\nvgg16.add(Convolution2D(filters=64,\n                       kernel_size=(3,3),\n                       padding='same',\n                       data_format='channels_last',\n                       activation='relu'))\n\n# Using max pool (4x4) as input to VGG16 model\nvgg16.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n \n# Adding second convolutional layer (3x3) as second input to VGG16 model\nvgg16.add(Convolution2D(filters=64,\n                       kernel_size=(3,3),\n                       padding='same',\n                       data_format='channels_last',\n                       activation='relu')) Cry is a type of bottleneck, for adding deeper depth (4x4) than the VGG16 model. \n# The number of filters and the kernel size are defined by 3x3 kernel size and 3x3 filter size.\nvgg16.add(Convolution2D(filters=64,\n                       kernel_size=(3,3),\n                       padding='same',\n                       data_format='channels_last',\n                       activation='relu'))\n\n# Flattening the resulting convnet\nvgg16.add(Flatten())\n\n# Adding output layer (classification)\nvgg16.add(Dense(units=512, activation='softmax'))\n\n# Compiling the network\nvgg16.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nvgg16\n","output_type":"stream"}]},{"cell_type":"markdown","source":"PROMPTING THE FULL DATASET IN THE SAME FORMAT AS THE ABOVE PROMPT","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset_1 = load_dataset(\"codeparrot/xlcost-text-to-code\", \"Python-program-level\")","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:30:47.155732Z","iopub.execute_input":"2023-09-14T15:30:47.156162Z","iopub.status.idle":"2023-09-14T15:30:52.611637Z","shell.execute_reply.started":"2023-09-14T15:30:47.156126Z","shell.execute_reply":"2023-09-14T15:30:52.610676Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.61k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b3d978940de4c5caec4d15f5256d915"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset xlcost/Python-program-level to /root/.cache/huggingface/datasets/codeparrot___xlcost/Python-program-level/2.1.0/ffae7d034dfaa9e215012bcf52b8690f3ae22d9c52f45fe2ffd3dcf4093d9f2c...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b6c1eb33f8a417da9546dc9ab5e7f5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/11.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6a1580c40fe49fba125123e41047ace"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.02M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b19059b6765340ab8e582f2a5c0e4d94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/570k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06fea11bc4bd4862995e320c6ce2e3e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset xlcost downloaded and prepared to /root/.cache/huggingface/datasets/codeparrot___xlcost/Python-program-level/2.1.0/ffae7d034dfaa9e215012bcf52b8690f3ae22d9c52f45fe2ffd3dcf4093d9f2c. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf997af9fa8c4e5cad6bf16fe4574f89"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_1","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:30:52.613363Z","iopub.execute_input":"2023-09-14T15:30:52.614283Z","iopub.status.idle":"2023-09-14T15:30:52.626683Z","shell.execute_reply.started":"2023-09-14T15:30:52.614245Z","shell.execute_reply":"2023-09-14T15:30:52.625688Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'code'],\n        num_rows: 9263\n    })\n    test: Dataset({\n        features: ['text', 'code'],\n        num_rows: 887\n    })\n    validation: Dataset({\n        features: ['text', 'code'],\n        num_rows: 472\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# special tokens for prompting\nsystem_token = \"<SYSTEM_TASK:>\"\nuser_token = \"<USER_TASK:>\"\n\ndef process_1(data):\n    text_prompts = list(map(lambda x: x.replace('|', ','), data['text']))\n    text_prompts = list(map(lambda x: x.replace('\\n', ','), data['text']))\n    prompt = ''\n    final_code = ''\n    final_text_prompt = []\n    final_code_prompt = []\n    code = data['code']\n    add_prompt = '<SYSTEM_TASK:>\\nSolve the following problem using Python, implementing the functions described below, one line at a time\\n<END_TASK>\\n<USER_TASK:>\\nDescription:\\n'\n    for i in range(len(text_prompts)):\n        prompt =  text_prompts[i]\n        new_prompt = add_prompt\n        arr = prompt.split(';')\n        for i in range(len(arr)):\n            arr[i] = arr[i].replace('|', '')\n            arr[i] = arr[i].lower()\n            new_prompt = new_prompt + f'def python_task_{i}():\\n' + f'\"\"\" This function {arr[i]}\"\"\"\\n'\n        code[i] = code[i].replace('NEW_LINE INDENT' , '\\n\\t')\n        code[i] = code[i].replace('NEW_LINE' , '\\n')\n        code[i] = code[i].replace('DEDENT' , '\\b')\n        code[i] = code[i].replace(' ( ' , '(')\n        code[i] = code[i].replace(' [ ', '[')\n        code[i] = code[i].replace('_ ', '_')\n        code[i] = code[i].replace('\" ', '\"')\n        final_code = code[i]\n        final_text_prompt.append(new_prompt)\n        final_code_prompt.append(final_code)\n    return {\n      'text_prompt' : final_text_prompt,\n      'code_prompt' : final_code_prompt\n    }","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:30:52.628351Z","iopub.execute_input":"2023-09-14T15:30:52.629091Z","iopub.status.idle":"2023-09-14T15:30:53.873864Z","shell.execute_reply.started":"2023-09-14T15:30:52.629055Z","shell.execute_reply":"2023-09-14T15:30:53.872762Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"final_text_prompt = []\nfinal_code_prompt = []\nprompted_data_1 = dataset_1.map(process_1, batched = True, remove_columns = ['text', 'code'])\nprompted_data_1","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:30:53.878190Z","iopub.execute_input":"2023-09-14T15:30:53.879699Z","iopub.status.idle":"2023-09-14T15:30:54.683710Z","shell.execute_reply.started":"2023-09-14T15:30:53.879652Z","shell.execute_reply":"2023-09-14T15:30:54.682692Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"153f42759b20480883cedefbbbedae8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e13beb58bf94e39b4bcadfe90ffb5a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d64ae69a99df4448be9d1140f219b3bf"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text_prompt', 'code_prompt'],\n        num_rows: 9263\n    })\n    test: Dataset({\n        features: ['text_prompt', 'code_prompt'],\n        num_rows: 887\n    })\n    validation: Dataset({\n        features: ['text_prompt', 'code_prompt'],\n        num_rows: 472\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(prompted_data_1['train'][24]['text_prompt'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:30:54.685355Z","iopub.execute_input":"2023-09-14T15:30:54.686049Z","iopub.status.idle":"2023-09-14T15:30:54.695520Z","shell.execute_reply.started":"2023-09-14T15:30:54.686010Z","shell.execute_reply":"2023-09-14T15:30:54.694607Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"<SYSTEM_TASK:>\nSolve the following problem using Python, implementing the functions described below, one line at a time\n<END_TASK>\n<USER_TASK:>\nDescription:\ndef python_task_0():\n\"\"\" This function count of repeating digits in a given number  function that returns the count of repeating digits of the given number \"\"\"\ndef python_task_1():\n\"\"\" This function  initialize a variable to store count of repeating digits \"\"\"\ndef python_task_2():\n\"\"\" This function  initialize cnt array to store digit count \"\"\"\ndef python_task_3():\n\"\"\" This function  iterate through the digits of n \"\"\"\ndef python_task_4():\n\"\"\" This function  retrieve the last digit of n \"\"\"\ndef python_task_5():\n\"\"\" This function  increase the count of digit \"\"\"\ndef python_task_6():\n\"\"\" This function  remove the last digit of n \"\"\"\ndef python_task_7():\n\"\"\" This function  iterate through the cnt array \"\"\"\ndef python_task_8():\n\"\"\" This function  if frequency of digit is greater than 1 \"\"\"\ndef python_task_9():\n\"\"\" This function  increment the count of repeating digits \"\"\"\ndef python_task_10():\n\"\"\" This function  return count of repeating digit \"\"\"\ndef python_task_11():\n\"\"\" This function  given array arr [ ] \"\"\"\ndef python_task_12():\n\"\"\" This function  function call\"\"\"\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(prompted_data_1['train'][24]['code_prompt'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:30:54.696954Z","iopub.execute_input":"2023-09-14T15:30:54.697516Z","iopub.status.idle":"2023-09-14T15:30:55.925334Z","shell.execute_reply.started":"2023-09-14T15:30:54.697480Z","shell.execute_reply":"2023-09-14T15:30:55.923557Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"mod = 1000000007 \n def ValOfTheExpression(n ) : \n\t global mod \n factorial =[0 for i in range(n + 1 ) ] \n factorial[0 ] = 1 \n factorial[1 ] = 1 \n for i in range(2 , n + 1 , 1 ) : \n\t factorial[i ] =(( factorial[i - 1 ] % mod ) *(i % mod ) ) % mod \n dp =[0 for i in range(n + 1 ) ] \n dp[1 ] = 1 \n for i in range(2 , n + 1 , 1 ) : \n\t dp[i ] =(( dp[i - 1 ] % mod ) *(factorial[i ] % mod ) ) % mod \n return dp[n ] \n if __name__== ' __main __' : \n\t n = 4 \n print(ValOfTheExpression(n ) ) \n\n","output_type":"stream"}]},{"cell_type":"code","source":"final_prompt = prompted_data_1['train'][111]['text_prompt']\ncode = prompted_data_1['train'][111]['code_prompt']\n\ngeneration_config = GenerationConfig(max_new_tokens=100, temperature= 0.8, do_sample = True, top_p = 3)\ninputs = tokenizer(final_prompt, return_tensors = 'pt')\n\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        generation_config = generation_config,\n        max_new_tokens = 100,\n        pad_token_id = tokenizer.pad_token_id,\n    )[0],\n    skip_special_tokens=True\n)\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{final_prompt}')\nprint(dash_line)\nprint(f'HUMAN BASELINE CODE:\\n{code}')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:32:10.972430Z","iopub.execute_input":"2023-09-14T15:32:10.973160Z","iopub.status.idle":"2023-09-14T15:32:16.239678Z","shell.execute_reply.started":"2023-09-14T15:32:10.973119Z","shell.execute_reply":"2023-09-14T15:32:16.238649Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1539: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n<SYSTEM_TASK:>\nSolve the following problem using Python, implementing the functions described below, one line at a time\n<END_TASK>\n<USER_TASK:>\nDescription:\ndef python_task_0():\n\"\"\" This function program to determine the quadrant of a complex number  function to determine the quadrant of a complex number \"\"\"\ndef python_task_1():\n\"\"\" This function  storing the index of '+ \"\"\"\ndef python_task_2():\n\"\"\" This function  storing the index of '- \"\"\"\ndef python_task_3():\n\"\"\" This function  finding the real part of the complex number \"\"\"\ndef python_task_4():\n\"\"\" This function  finding the imaginary part of the complex number \"\"\"\ndef python_task_5():\n\"\"\" This function  driver code\"\"\"\n\n---------------------------------------------------------------------------------------------------\nHUMAN BASELINE CODE:\ndef sameProductQuadruples(nums , N ) : \n\t umap = { } ; \n res = 0 ; \n for i in range(N ) : \n\t for j in range(i + 1 , N ) : \n\t prod = nums[i ] * nums[j ] ; \n if prod in umap : \n\t res += 8 * umap[prod ] ; \n umap[prod ] += 1 ; \n else : \n\t umap[prod ] = 1 \n print(res ) ; \n if __name__== \"__main __\": \n\t arr =[2 , 3 , 4 , 6 ] ; \n N = len(arr ) ; \n sameProductQuadruples(arr , N ) ; \n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n\nSolve the following problem using Python, implementing the functions described below, one line at a time\n\n\nDescription:\ndef python_task_0():\n\"\"\" This function program to determine the quadrant of a complex number  function to determine the quadrant of a complex number \"\"\"\ndef python_task_1():\n\"\"\" This function  storing the index of '+ \"\"\"\ndef python_task_2():\n\"\"\" This function  storing the index of '- \"\"\"\ndef python_task_3():\n\"\"\" This function  finding the real part of the complex number \"\"\"\ndef python_task_4():\n\"\"\" This function  finding the imaginary part of the complex number \"\"\"\ndef python_task_5():\n\"\"\" This function  driver code\"\"\"\ndef python_task_6():\n\"\"\" This function  recreating the function\"\"\"\ndef python_task_7():\n\"\"\" This function  implementing the fact\"\"\"\ndef python_task_8():\n\"\"\"This function  creating a matrix for the function to run it on\"\"\"\ndef python_task_9():\n\"\"\" This function  computing the fact\"\"\"\ndef python_task_10():\n\"\"\" This function  performing the fact\"\"\"\ndef python_\n","output_type":"stream"}]},{"cell_type":"code","source":"! pip install git+https://github.com/huggingface/datasets#egg=datasets","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:54:37.734356Z","iopub.execute_input":"2023-09-14T15:54:37.735267Z","iopub.status.idle":"2023-09-14T15:55:10.175081Z","shell.execute_reply.started":"2023-09-14T15:54:37.735235Z","shell.execute_reply":"2023-09-14T15:55:10.173773Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Collecting datasets\n  Cloning https://github.com/huggingface/datasets to /tmp/pip-install-k6bkleq3/datasets_9f2cc6ba46184cdc8916ac1967f2d857\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/datasets /tmp/pip-install-k6bkleq3/datasets_9f2cc6ba46184cdc8916ac1967f2d857\n  Resolved https://github.com/huggingface/datasets to commit a6fb8b9a833afb25311da395c6e0d9bf770ca2c7\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nCollecting fsspec[http]<2023.9.0,>=2023.1.0 (from datasets)\n  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nBuilding wheels for collected packages: datasets\n  Building wheel for datasets (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for datasets: filename=datasets-2.14.6.dev0-py3-none-any.whl size=493354 sha256=ba304f57c06bf26da72eedff7dc21028078e7cc28d6e8f46e077fdd471d0c22d\n  Stored in directory: /tmp/pip-ephem-wheel-cache-2kzzmzhf/wheels/7f/ba/ce/8f6a52388a9966c7d9afa987113a763f7c105f568f369adbc6\nSuccessfully built datasets\nInstalling collected packages: fsspec, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2023.9.0\n    Uninstalling fsspec-2023.9.0:\n      Successfully uninstalled fsspec-2023.9.0\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\ns3fs 2023.9.0 requires fsspec==2023.9.0, but you have fsspec 2023.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.14.6.dev0 fsspec-2023.6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset_2 = load_dataset(\"code_x_glue_ct_code_to_text\", \"python\", download_mode=\"force_redownload\")","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:55:10.177236Z","iopub.execute_input":"2023-09-14T15:55:10.177648Z","iopub.status.idle":"2023-09-14T16:00:32.609033Z","shell.execute_reply.started":"2023-09-14T15:55:10.177608Z","shell.execute_reply":"2023-09-14T16:00:32.608089Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d944d2ab24fc4589ad8d7ec7757c844c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/17.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab5e64dc6c964c38a1a10c2ee0a8c13b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/25.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29e7dbdefc094c8581c27d56b480b300"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/2.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93529eff00ea47428eb1882c1c42d4cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.74k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc2b22f0556e4f80989e0cd9ef7cb9eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5c34be1a16749baa0744625ddac3fe8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/941M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7b8305ebed941aea75655f289773252"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/12.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3db35e9d5ddc401b95b9ac5363ceab94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94d32224d1dd4c89bede2e9e860a6549"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c89e8f39ec84d0da136edf219ee14be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/941M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c8ffd96f591428db7c0a1513994b830"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/12.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99bc7e45ee3f4cde9a2ef23e26763806"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb0669ce614b4f0c85d6a7b03f2954b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0821976e3614eb6a7213d5d35384e9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/941M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e449d70bdba94b4c9cfa31cf698dd1f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/12.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6a7f067bc7c4958b3e1207c449417a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e8cac4349e64269819c2bd7bdce39ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/251820 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de6d8a57653c4fb7943dc853686f2c63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/13914 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b80f971df434359b5d010e1eb4a3af7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/14918 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"facc5b2e389b4d3382b659adf62a5c5b"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_2","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:01:46.793694Z","iopub.execute_input":"2023-09-14T16:01:46.794880Z","iopub.status.idle":"2023-09-14T16:01:46.801898Z","shell.execute_reply.started":"2023-09-14T16:01:46.794832Z","shell.execute_reply":"2023-09-14T16:01:46.800824Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url'],\n        num_rows: 251820\n    })\n    validation: Dataset({\n        features: ['id', 'repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url'],\n        num_rows: 13914\n    })\n    test: Dataset({\n        features: ['id', 'repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url'],\n        num_rows: 14918\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(dataset_2['train'][91]['code'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:01:47.049369Z","iopub.execute_input":"2023-09-14T16:01:47.049949Z","iopub.status.idle":"2023-09-14T16:01:47.060886Z","shell.execute_reply.started":"2023-09-14T16:01:47.049918Z","shell.execute_reply":"2023-09-14T16:01:47.059880Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"def tag(self, tag):\n        \"\"\"Get a release by tag\n        \"\"\"\n        url = '%s/tags/%s' % (self, tag)\n        response = self.http.get(url, auth=self.auth)\n        response.raise_for_status()\n        return response.json()\n","output_type":"stream"}]},{"cell_type":"code","source":"print(dataset_2['train'][91]['docstring'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:01:47.303798Z","iopub.execute_input":"2023-09-14T16:01:47.304210Z","iopub.status.idle":"2023-09-14T16:01:47.310368Z","shell.execute_reply.started":"2023-09-14T16:01:47.304180Z","shell.execute_reply":"2023-09-14T16:01:47.309225Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Get a release by tag\n","output_type":"stream"}]},{"cell_type":"code","source":"def dataset_formation(data):\n    add_prompt = '<SYSTEM_TASK:>\\nSolve the following problem using Python, implementing the functions described below, one line at a time\\n<END_TASK>\\n<USER_TASK:>\\nDescription:\\n'\n    doc = data['docstring'].split('\\n')\n    cleaned_doc = []\n    if len(doc) != 0:\n        for i in range(len(doc)):\n            if ':' in doc[i]:\n                pass\n            else:\n                cleaned_doc.append(doc[i])\n    final_text = ''\n    for i in range(len(cleaned_doc)):\n        final_text = final_text + ' ' + cleaned_doc[i]\n    s = data['code']\n    code = ''\n    final_prompt = ''\n    occurrences = re.finditer('\"\"\"', s)\n    # using reduce() to get start indices of all occurrences\n    res = reduce(lambda x, y: x + [y.start()], occurrences, [])\n    extracted_prompt = ''\n    if len(res) > 1:     \n        pos = res[1]\n        prompt = data['code'][:pos+3]\n        lines = prompt.split('\\n')\n        cleaned_extracted_prompt = []\n        for i in range(len(lines)):\n            if '>>>' in lines[i] or '...' in lines[i] or '----------' in lines[i]:\n                pass\n            else:\n                cleaned_extracted_prompt.append(lines[i])\n        extracted_prompt = ''\n        for j in range(len(cleaned_extracted_prompt)):\n            extracted_prompt = extracted_prompt + ' ' + cleaned_extracted_prompt[j]\n        extracted_prompt = re.sub(' +', ' ', extracted_prompt)\n        extracted_prompt = extracted_prompt.replace('):' , '):\\n')\n        code = s.replace(prompt, '')\n    else:\n        code = data['code']\n    \n    final_prompt = add_prompt + extracted_prompt\n    return{\n        'text_prompt' : final_prompt,\n        'code_prompt' : code\n    }","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:01:48.784194Z","iopub.execute_input":"2023-09-14T16:01:48.784999Z","iopub.status.idle":"2023-09-14T16:01:48.797982Z","shell.execute_reply.started":"2023-09-14T16:01:48.784945Z","shell.execute_reply":"2023-09-14T16:01:48.796981Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"prompted_dataset_2 = dataset_2.map(dataset_formation, batched = False, remove_columns = dataset_2['train'].column_names)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:01:49.981206Z","iopub.execute_input":"2023-09-14T16:01:49.982019Z","iopub.status.idle":"2023-09-14T16:02:37.691893Z","shell.execute_reply.started":"2023-09-14T16:01:49.981978Z","shell.execute_reply":"2023-09-14T16:02:37.690872Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/251820 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"415b11837dcf4164b88ebc5c613ad70d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13914 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b61eff4537fa46f9987e80bc0145c959"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14918 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82db9acac30e4242a9005bf3992345c8"}},"metadata":{}}]},{"cell_type":"code","source":"prompted_dataset_2","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:02:37.694090Z","iopub.execute_input":"2023-09-14T16:02:37.694731Z","iopub.status.idle":"2023-09-14T16:02:37.701571Z","shell.execute_reply.started":"2023-09-14T16:02:37.694693Z","shell.execute_reply":"2023-09-14T16:02:37.700491Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text_prompt', 'code_prompt'],\n        num_rows: 251820\n    })\n    validation: Dataset({\n        features: ['text_prompt', 'code_prompt'],\n        num_rows: 13914\n    })\n    test: Dataset({\n        features: ['text_prompt', 'code_prompt'],\n        num_rows: 14918\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(prompted_dataset_2['train'][91]['text_prompt'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:02:37.703062Z","iopub.execute_input":"2023-09-14T16:02:37.703666Z","iopub.status.idle":"2023-09-14T16:02:37.729254Z","shell.execute_reply.started":"2023-09-14T16:02:37.703632Z","shell.execute_reply":"2023-09-14T16:02:37.728304Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"<SYSTEM_TASK:>\nSolve the following problem using Python, implementing the functions described below, one line at a time\n<END_TASK>\n<USER_TASK:>\nDescription:\n def tag(self, tag):\n \"\"\"Get a release by tag \"\"\"\n","output_type":"stream"}]},{"cell_type":"code","source":"print(prompted_dataset_2['train'][91]['code_prompt'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:02:37.732226Z","iopub.execute_input":"2023-09-14T16:02:37.732725Z","iopub.status.idle":"2023-09-14T16:02:37.741355Z","shell.execute_reply.started":"2023-09-14T16:02:37.732690Z","shell.execute_reply":"2023-09-14T16:02:37.740302Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"\n        url = '%s/tags/%s' % (self, tag)\n        response = self.http.get(url, auth=self.auth)\n        response.raise_for_status()\n        return response.json()\n","output_type":"stream"}]},{"cell_type":"code","source":"final_prompt = prompted_dataset_2['train'][1435]['text_prompt']\ncode = prompted_dataset_2['train'][1435]['code_prompt']\n\ngeneration_config = GenerationConfig(max_new_tokens=500, temperature= 1.2, do_sample = True, top_p = 3)\ninputs = tokenizer(final_prompt, return_tensors = 'pt')\n\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        generation_config = generation_config,\n        max_new_tokens = 500,\n        pad_token_id = tokenizer.pad_token_id,\n    )[0],\n    skip_special_tokens=True\n)\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{final_prompt}')\nprint(dash_line)\nprint(f'HUMAN BASELINE CODE:\\n{code}')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"execution":{"iopub.status.busy":"2023-09-14T10:32:20.521742Z","iopub.execute_input":"2023-09-14T10:32:20.522079Z","iopub.status.idle":"2023-09-14T10:32:37.508794Z","shell.execute_reply.started":"2023-09-14T10:32:20.522048Z","shell.execute_reply":"2023-09-14T10:32:37.507784Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1539: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n<SYSTEM_TASK:>\nSolve the following problem using Python, implementing the functions described below, one line at a time\n<END_TASK>\n<USER_TASK:>\nDescription:\n def _get_data(self, time, site_id):\n r\"\"\"Download and parse upper air observations from an online archive. Parameters time : datetime The date and time of the desired observation. site_id : str The three letter ICAO identifier of the station for which data should be downloaded. Returns ------- :class:`pandas.DataFrame` containing the data \"\"\"\n---------------------------------------------------------------------------------------------------\nHUMAN BASELINE CODE:\n\n        raw_data = self._get_data_raw(time, site_id)\n        soup = BeautifulSoup(raw_data, 'html.parser')\n        tabular_data = StringIO(soup.find_all('pre')[0].contents[0])\n        col_names = ['pressure', 'height', 'temperature', 'dewpoint', 'direction', 'speed']\n        df = pd.read_fwf(tabular_data, skiprows=5, usecols=[0, 1, 2, 3, 6, 7], names=col_names)\n        df['u_wind'], df['v_wind'] = get_wind_components(df['speed'],\n                                                         np.deg2rad(df['direction']))\n\n        # Drop any rows with all NaN values for T, Td, winds\n        df = df.dropna(subset=('temperature', 'dewpoint', 'direction', 'speed',\n                               'u_wind', 'v_wind'), how='all').reset_index(drop=True)\n\n        # Parse metadata\n        meta_data = soup.find_all('pre')[1].contents[0]\n        lines = meta_data.splitlines()\n\n        # If the station doesn't have a name identified we need to insert a\n        # record showing this for parsing to proceed.\n        if 'Station number' in lines[1]:\n            lines.insert(1, 'Station identifier: ')\n\n        station = lines[1].split(':')[1].strip()\n        station_number = int(lines[2].split(':')[1].strip())\n        sounding_time = datetime.strptime(lines[3].split(':')[1].strip(), '%y%m%d/%H%M')\n        latitude = float(lines[4].split(':')[1].strip())\n        longitude = float(lines[5].split(':')[1].strip())\n        elevation = float(lines[6].split(':')[1].strip())\n\n        df['station'] = station\n        df['station_number'] = station_number\n        df['time'] = sounding_time\n        df['latitude'] = latitude\n        df['longitude'] = longitude\n        df['elevation'] = elevation\n\n        # Add unit dictionary\n        df.units = {'pressure': 'hPa',\n                    'height': 'meter',\n                    'temperature': 'degC',\n                    'dewpoint': 'degC',\n                    'direction': 'degrees',\n                    'speed': 'knot',\n                    'u_wind': 'knot',\n                    'v_wind': 'knot',\n                    'station': None,\n                    'station_number': None,\n                    'time': None,\n                    'latitude': 'degrees',\n                    'longitude': 'degrees',\n                    'elevation': 'meter'}\n        return df\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n\nSolve the following problem using Python, implementing the functions described below, one line at a time\n\n\nDescription:\n def _get_data(self, time, site_id):\n r\"\"\"Download and parse upper air observations from an online archive. Parameters time : datetime The date and time of the desired observation. site_id : str The three letter ICAO identifier of the station for which data should be downloaded. Returns ------- :class:`pandas.DataFrame` containing the data \"\"\"\n import requests\n get_data = requests.get(\"http://tmos.ucsb.edu/cgi/nmt_index/index.bib\").content\n pd.read_csv(io.StringIO(get_data.decode('utf-8')))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**The above dataset is working GOOD**","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset_3 = load_dataset(\"openai_humaneval\")","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:02:37.742665Z","iopub.execute_input":"2023-09-14T16:02:37.742971Z","iopub.status.idle":"2023-09-14T16:02:39.255695Z","shell.execute_reply.started":"2023-09-14T16:02:37.742948Z","shell.execute_reply":"2023-09-14T16:02:39.254698Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/3.28k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d64f5b8b6a71444b8e5effb37e64b55c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/3.33k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0b8e042d289435c9ff0ce956343e9e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/6.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87eab9f5efc34d4ebaab11b4eed21fe1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/44.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8404f4c50984900aff32071184b7d1b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/164 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4de0f1daf85d4504b68fb7cbf6df89a2"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_3","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:02:39.257172Z","iopub.execute_input":"2023-09-14T16:02:39.258225Z","iopub.status.idle":"2023-09-14T16:02:39.265127Z","shell.execute_reply.started":"2023-09-14T16:02:39.258189Z","shell.execute_reply":"2023-09-14T16:02:39.264131Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    test: Dataset({\n        features: ['task_id', 'prompt', 'canonical_solution', 'test', 'entry_point'],\n        num_rows: 164\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(dataset_3['test'][41]['prompt'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:02:39.266714Z","iopub.execute_input":"2023-09-14T16:02:39.267711Z","iopub.status.idle":"2023-09-14T16:02:39.281651Z","shell.execute_reply.started":"2023-09-14T16:02:39.267679Z","shell.execute_reply":"2023-09-14T16:02:39.280882Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"\n\ndef car_race_collision(n: int):\n    \"\"\"\n    Imagine a road that's a perfectly straight infinitely long line.\n    n cars are driving left to right;  simultaneously, a different set of n cars\n    are driving right to left.   The two sets of cars start out being very far from\n    each other.  All cars move in the same speed.  Two cars are said to collide\n    when a car that's moving left to right hits a car that's moving right to left.\n    However, the cars are infinitely sturdy and strong; as a result, they continue moving\n    in their trajectory as if they did not collide.\n\n    This function outputs the number of such collisions.\n    \"\"\"\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(dataset_3['test'][10]['canonical_solution'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:02:39.283052Z","iopub.execute_input":"2023-09-14T16:02:39.284179Z","iopub.status.idle":"2023-09-14T16:02:39.293149Z","shell.execute_reply.started":"2023-09-14T16:02:39.284143Z","shell.execute_reply":"2023-09-14T16:02:39.292121Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"    if not string:\n        return ''\n\n    beginning_of_suffix = 0\n\n    while not is_palindrome(string[beginning_of_suffix:]):\n        beginning_of_suffix += 1\n\n    return string + string[:beginning_of_suffix][::-1]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"def dataset_formation(data):\n    add_prompt = '<SYSTEM_TASK:>\\nSolve the following problem using Python, implementing the functions described below, one line at a time\\n<END_TASK>\\n<USER_TASK:>\\nDescription:\\n'\n    prompt = data['prompt']\n    prompt = prompt.replace('\"\"\"\\n', '\"\"\"')\n    prompt = prompt.replace('\\n\"\"\"', '\"\"\"')\n    prompt_copy = prompt\n    lines = prompt.split('\\n')\n    occurrences = re.finditer('\"\"\"', prompt)\n    res = reduce(lambda x, y: x + [y.start()], occurrences, [])\n    final_cleaned = ''\n    if len(res) != 0:\n        for i in range(int(len(res)/2)):\n            cleaned_prompt = ''\n            clnd = []\n            line = prompt[res[2*i]+ 3 : res[2*i - 1]].split('\\n')\n            for j in range(len(line)):\n                if ':' in line[j] or '*' in line[j] or '>>>' in line[j] or '=>' in line[j] or '->' in line[j]:\n                    pass\n                else:\n                    clnd.append(line[j])\n            for k in range(len(clnd)):\n                cleaned_prompt = cleaned_prompt + clnd[k] + '\\n'\n            cleaned_prompt = cleaned_prompt.replace('\\n', '')\n            cleaned_prompt = re.sub(' +', ' ', cleaned_prompt)\n            final_cleaned = prompt_copy.replace(prompt[res[i] + 3 : res[2*i - 1]], cleaned_prompt)\n    if final_cleaned == '':\n        final_prompt = None\n    else:\n        final_prompt = add_prompt + final_cleaned\n    code = data['canonical_solution']\n    return{\n        'text_prompt' : final_prompt,\n        'code_prompt' : code\n    }","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:02:39.295818Z","iopub.execute_input":"2023-09-14T16:02:39.296401Z","iopub.status.idle":"2023-09-14T16:02:39.307909Z","shell.execute_reply.started":"2023-09-14T16:02:39.296369Z","shell.execute_reply":"2023-09-14T16:02:39.306856Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"prompted_dataset_3 = dataset_3.map(dataset_formation, batched = False, remove_columns = dataset_3['test'].column_names)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:02:39.311866Z","iopub.execute_input":"2023-09-14T16:02:39.312255Z","iopub.status.idle":"2023-09-14T16:02:39.370854Z","shell.execute_reply.started":"2023-09-14T16:02:39.312229Z","shell.execute_reply":"2023-09-14T16:02:39.369959Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/164 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a2b4114d40e4aa386f48a4f3c482548"}},"metadata":{}}]},{"cell_type":"code","source":"prompted_dataset_3","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:02:39.372182Z","iopub.execute_input":"2023-09-14T16:02:39.372753Z","iopub.status.idle":"2023-09-14T16:02:39.378679Z","shell.execute_reply.started":"2023-09-14T16:02:39.372720Z","shell.execute_reply":"2023-09-14T16:02:39.377726Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    test: Dataset({\n        features: ['text_prompt', 'code_prompt'],\n        num_rows: 164\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(prompted_dataset_3['test'][114]['text_prompt'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:02:39.380141Z","iopub.execute_input":"2023-09-14T16:02:39.380834Z","iopub.status.idle":"2023-09-14T16:02:39.393713Z","shell.execute_reply.started":"2023-09-14T16:02:39.380784Z","shell.execute_reply":"2023-09-14T16:02:39.392797Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"<SYSTEM_TASK:>\nSolve the following problem using Python, implementing the functions described below, one line at a time\n<END_TASK>\n<USER_TASK:>\nDescription:\n\ndef minSubArraySum(nums):\n    \"\"\" Given an array of integers nums, find the minimum sum of any non-empty sub-array of nums. Example minSubArraySum([2, 3, 4, 1, 2, 4]) == 1 minSubArraySum([-1, -2, -3]) == -6 \"\"\"\n","output_type":"stream"}]},{"cell_type":"code","source":"print(prompted_dataset_3['test'][114]['code_prompt'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:02:39.395155Z","iopub.execute_input":"2023-09-14T16:02:39.395940Z","iopub.status.idle":"2023-09-14T16:02:39.406334Z","shell.execute_reply.started":"2023-09-14T16:02:39.395907Z","shell.execute_reply":"2023-09-14T16:02:39.405297Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"    max_sum = 0\n    s = 0\n    for num in nums:\n        s += -num\n        if (s < 0):\n            s = 0\n        max_sum = max(s, max_sum)\n    if max_sum == 0:\n        max_sum = max(-i for i in nums)\n    min_sum = -max_sum\n    return min_sum\n\n","output_type":"stream"}]},{"cell_type":"code","source":"final_prompt = prompted_dataset_3['test'][114]['text_prompt']\ncode = prompted_dataset_3['test'][114]['code_prompt']\ninputs = tokenizer(final_prompt, return_tensors = 'pt').to('cuda')\ngeneration_config = GenerationConfig(max_new_tokens=250, temperature= 0.9, do_sample = True, top_p = 3)\n\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens = 250,\n        generation_config=generation_config,\n        pad_token_id = tokenizer.pad_token_id,\n    )[0],\n    skip_special_tokens=True\n)\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{final_prompt}')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')\nprint(dash_line)\nprint(f'BASELINE HUMAN WRITTEN CODE:\\n{code}')","metadata":{"execution":{"iopub.status.busy":"2023-09-14T12:11:17.985479Z","iopub.execute_input":"2023-09-14T12:11:17.985958Z","iopub.status.idle":"2023-09-14T12:11:27.638993Z","shell.execute_reply.started":"2023-09-14T12:11:17.985922Z","shell.execute_reply":"2023-09-14T12:11:27.638093Z"},"trusted":true},"execution_count":114,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n<SYSTEM_TASK:>\nSolve the following problem using Python, implementing the functions described below, one line at a time\n<END_TASK>\n<USER_TASK:>\nDescription:\n\ndef minSubArraySum(nums):\n    \"\"\" Given an array of integers nums, find the minimum sum of any non-empty sub-array of nums. Example minSubArraySum([2, 3, 4, 1, 2, 4]) == 1 minSubArraySum([-1, -2, -3]) == -6 \"\"\"\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n\nSolve the following problem using Python, implementing the functions described below, one line at a time\n\n\nDescription:\n\ndef minSubArraySum(nums):\n    \"\"\" Given an array of integers nums, find the minimum sum of any non-empty sub-array of nums. Example minSubArraySum([2, 3, 4, 1, 2, 4]) == 1 minSubArraySum([-1, -2, -3]) == -6 \"\"\"\n    # Your code here\n    if not nums:\n        return 0\n    minimum = float('inf')\n    answer = float('inf')\n    start = 0\n    end = start\n    for i in range(len(nums)):\n        if nums[i] - 1 < 0:\n            answer = min(answer, nums[i] - 1)\n        else:\n            if nums[i] - 1 < minimum:\n                minimum = nums[i] - 1\n            if nums[i] - nums[i + 1] <= minimum:\n                answer = min(answer, nums[i] - minimum)\n            end = i\n    if end == len(nums):\n        answer = min(answer, 0)\n    return answer\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN WRITTEN CODE:\n    max_sum = 0\n    s = 0\n    for num in nums:\n        s += -num\n        if (s < 0):\n            s = 0\n        max_sum = max(s, max_sum)\n    if max_sum == 0:\n        max_sum = max(-i for i in nums)\n    min_sum = -max_sum\n    return min_sum\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Working Good**","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset_4 = load_dataset(\"codeparrot/github-jupyter-code-to-text\")","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:02:41.416891Z","iopub.execute_input":"2023-09-14T16:02:41.417626Z","iopub.status.idle":"2023-09-14T16:02:55.249784Z","shell.execute_reply.started":"2023-09-14T16:02:41.417592Z","shell.execute_reply":"2023-09-14T16:02:55.248831Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/857 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bc3451e6ad14c678c10d608ec8b38ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69057fe587854dd29473b90b4b7886f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/227M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cf8e00773374fa6bc501233b4b87a90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/56.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b51054ada2a48099ebfbeedd6b51add"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac45be953b8340298a303a8488eef849"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7455616131564ca4b5a5e3b17851112d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3389ccfc3af94c7fbe5af55dc162d8b0"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_4","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:02:55.254548Z","iopub.execute_input":"2023-09-14T16:02:55.257375Z","iopub.status.idle":"2023-09-14T16:02:55.267869Z","shell.execute_reply.started":"2023-09-14T16:02:55.257337Z","shell.execute_reply":"2023-09-14T16:02:55.266873Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['repo_name', 'path', 'license', 'content'],\n        num_rows: 47452\n    })\n    test: Dataset({\n        features: ['repo_name', 'path', 'license', 'content'],\n        num_rows: 11864\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(dataset_4['train'][23]['path'])    # useful\nprint(dataset_4['train'][23]['repo_name'])\nprint(dataset_4['train'][23]['license'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:02:55.269293Z","iopub.execute_input":"2023-09-14T16:02:55.270049Z","iopub.status.idle":"2023-09-14T16:02:55.422103Z","shell.execute_reply.started":"2023-09-14T16:02:55.270009Z","shell.execute_reply":"2023-09-14T16:02:55.420978Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"lucid_work/notebooks/feature_visualization.ipynb\ndavidparks21/qso_lya_detection_pipeline\nmit\n","output_type":"stream"}]},{"cell_type":"code","source":"print(dataset_4['train'][263]['content'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:02:55.425546Z","iopub.execute_input":"2023-09-14T16:02:55.427914Z","iopub.status.idle":"2023-09-14T16:02:55.437154Z","shell.execute_reply.started":"2023-09-14T16:02:55.427880Z","shell.execute_reply":"2023-09-14T16:02:55.436190Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"%run ../bst/bst.py\n%load ../bst/bst.py\n\ndef height(node):\n    # TODO: Implement me\n    pass\n\n\"\"\"\nExplanation: <small><i>This notebook was prepared by Donne Martin. Source and license info is on GitHub.</i></small>\nChallenge Notebook\nProblem: Determine the height of a tree.\n\nConstraints\nTest Cases\nAlgorithm\nCode\nUnit Test\nSolution Notebook\n\nConstraints\n\nIs this a binary tree?\nYes\n\n\nCan we assume we already have a Node class with an insert method?\nYes\n\n\n\nTest Cases\n\n5 -> 1\n5, 2, 8, 1, 3 -> 3\n\nAlgorithm\nRefer to the Solution Notebook.  If you are stuck and need a hint, the solution notebook's algorithm discussion might be a good place to start.\nCode\nEnd of explanation\n\"\"\"\n\n\n# %load test_height.py\nfrom nose.tools import assert_equal\n\n\nclass TestHeight(object):\n\n    def test_height(self):\n        root = Node(5)\n        assert_equal(height(root), 1)\n        insert(root, 2)\n        insert(root, 8)\n        insert(root, 1)\n        insert(root, 3)\n        assert_equal(height(root), 3)\n\n        print('Success: test_height')\n\n\ndef main():\n    test = TestHeight()\n    test.test_height()\n\n\nif __name__ == '__main__':\n    main()\n\n\"\"\"\nExplanation: Unit Test\nThe following unit test is expected to fail until you solve the challenge.\nEnd of explanation\n\"\"\"\n\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import fasttext\nfrom huggingface_hub import hf_hub_download\n\nmodel_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\ndetect = fasttext.load_model(model_path)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:02:55.438608Z","iopub.execute_input":"2023-09-14T16:02:55.439645Z","iopub.status.idle":"2023-09-14T16:03:03.402515Z","shell.execute_reply.started":"2023-09-14T16:02:55.439612Z","shell.execute_reply":"2023-09-14T16:03:03.400874Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.bin:   0%|          | 0.00/1.18G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"035563469f3f488fbf1567afd205d9e0"}},"metadata":{}},{"name":"stderr","text":"Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n","output_type":"stream"}]},{"cell_type":"code","source":"def detect_lang(data):\n    lines = data.split('\\n')\n    text = ''\n    for i in range(len(lines)):\n        if i < 4:\n            text = text + lines[i]\n        else:\n            break\n    lang = detect.predict(text)[0][0]\n    if lang == '__label__eng_Latn':\n        return 'en'\n    else:\n        return ''","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:03:03.409535Z","iopub.execute_input":"2023-09-14T16:03:03.412135Z","iopub.status.idle":"2023-09-14T16:03:03.425119Z","shell.execute_reply.started":"2023-09-14T16:03:03.412088Z","shell.execute_reply":"2023-09-14T16:03:03.424447Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def dataset_formation(data):\n    add_prompt = '<SYSTEM_TASK:>\\nSolve the following problem using Python, implementing the functions described below, one line at a time\\n<END_TASK>\\n<USER_TASK:>\\nDescription:\\n'\n    s = data['content']\n    dummy = data['content']\n    occurrences = re.finditer('\"\"\"', s)\n    # using reduce() to get start indices of all occurrences\n    res = reduce(lambda x, y: x + [y.start()], occurrences, [])\n    extracted_prompt_arr = []\n    extracted_code = ''\n    final_prompt = ''\n    code = ''\n    if len(res)%2 == 0:\n        for i in range(int(len(res)/2)):\n            extracted_prompt_arr.append(s[res[2*i] + 3 : res[2*i + 1]] + '\\n')\n            dummy = dummy.replace(s[res[2*i] : res[2*i + 1] + 3], '')\n        final = ''\n        extracted_code = dummy\n        # LANGUAGE DETECTION  \n        lang = detect_lang(extracted_prompt_arr[0])\n        if lang == 'en':\n            final_1, final_2 = '', ''\n            for i in range(len(extracted_prompt_arr)):\n                    exp = extracted_prompt_arr[i]\n                    occurrences_1 = re.finditer('Explanation:', exp)\n                    # using reduce() to get start indices of all occurrences\n                    start = reduce(lambda x, y: x + [y.start()], occurrences_1, [])\n                    occurrences_2 = re.finditer('End of explanation', exp)\n                    end = reduce(lambda x, y: x + [y.start()], occurrences_2, [])\n                    if len(start) != 0 and len(end) != 0:\n                        extracted_exp = exp[start[0] + 12 : end[0]]\n                        final_1 = final_1 + extracted_exp + '\\n'\n            final_2 = final_1\n            occurrences_3 = re.finditer('<', final_1)\n            start = reduce(lambda x, y: x + [y.start()], occurrences_3, [])   \n            occurrences_4 = re.finditer('>', final_1)\n            end = reduce(lambda x, y: x + [y.start()], occurrences_4, [])\n            if len(start) == len(end) and len(start) != 0:\n                for i in range(len(start)):\n                    final_2 = final_2.replace(final_1[start[i] : end[i]], '')\n            else:\n                final_2 = final_1\n            final_3 = final_2\n            occurrences_5 = re.finditer('<', final_2)\n            start = reduce(lambda x, y: x + [y.start()], occurrences_5, [])   \n            occurrences_6 = re.finditer('>', final_2)\n            end = reduce(lambda x, y: x + [y.start()], occurrences_6, [])\n            if len(start) == len(end) and len(start) != 0:\n                for i in range(len(start)):\n                    final_3 = final_3.replace(final_2[start[i] : end[i]], '')\n            else:\n                final_3 = final_2\n            final = final_3 \n            if final != '':\n                final_line = ''\n                arr = final.split('\\n')\n                for i in range(len(arr)):\n                    if i < 9:\n                         if ':' not in arr[i] or '*' not in arr[i] or '>>>' not in arr[i] or '=>' not in arr[i] or '->' not in arr[i] or '>' not in arr[i] or '**' not in arr[i]:\n                            final_line = final_line + arr[i] + ' '\n                # LANGUAGE DETECTION  2\n                lang = detect_lang(final_line)\n                if lang == 'en':\n                    path = data['path'].split('/')\n                    func = path[-1].split('.')\n                    final_prompt = add_prompt + f'def {func[0]}():' + '\\n\\t' + f'\"\"\"{final_line}\"\"\"'   \n                else:\n                    final_prompt = None\n                lines = extracted_code.split('\\n')\n                code = ''\n                for i in range(len(lines)):\n                    if len(lines[i]) != 0:\n                        if lines[i][0] != '#' or '--' not in lines[i] or '!' not in lines[i] or '#' not in lines[i]:\n                            code = code + lines[i] + '\\n'\n                        elif 'EOF' in lines[i]:\n                            break\n                    \n            \n    else:\n        print('Error ^_^')\n    \n    return{\n        'text_prompt' : final_prompt,\n        'code_prompt' : code\n    }   ","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:03:03.426378Z","iopub.execute_input":"2023-09-14T16:03:03.426979Z","iopub.status.idle":"2023-09-14T16:03:04.531873Z","shell.execute_reply.started":"2023-09-14T16:03:03.426942Z","shell.execute_reply":"2023-09-14T16:03:04.530772Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"prompted_dataset_4 = dataset_4.map(dataset_formation, batched = False, remove_columns = dataset_4['train'].column_names)\n\nprompted_dataset_4","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:03:04.533577Z","iopub.execute_input":"2023-09-14T16:03:04.533976Z","iopub.status.idle":"2023-09-14T16:04:37.627518Z","shell.execute_reply.started":"2023-09-14T16:03:04.533939Z","shell.execute_reply":"2023-09-14T16:04:37.626486Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"Parameter 'function'=<function dataset_formation at 0x797c8d456a70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/47452 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06a3ec46b7e04179aa1e00fdf5a984cb"}},"metadata":{}},{"name":"stdout","text":"Error ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11864 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d368cad12ab14fa48a36d30196d0d701"}},"metadata":{}},{"name":"stdout","text":"Error ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\nError ^_^\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text_prompt', 'code_prompt'],\n        num_rows: 47452\n    })\n    test: Dataset({\n        features: ['text_prompt', 'code_prompt'],\n        num_rows: 11864\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"print(prompted_dataset_4['train'][114]['text_prompt'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:04:37.629297Z","iopub.execute_input":"2023-09-14T16:04:37.629985Z","iopub.status.idle":"2023-09-14T16:04:37.637071Z","shell.execute_reply.started":"2023-09-14T16:04:37.629948Z","shell.execute_reply":"2023-09-14T16:04:37.635891Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"<SYSTEM_TASK:>\nSolve the following problem using Python, implementing the functions described below, one line at a time\n<END_TASK>\n<USER_TASK:>\nDescription:\ndef popsizes():\n\t\"\"\" Example: modeling changes in population size Simple example Let's look at an example:   A simple bottleneck In order to change population size, one simply has to change the values in the \"nlist\".   For example, here is a population bottleneck:   Please note the last command, which changes the concatenated array from an array of 64 bit signed integers to 32 bit unsigned integers. Exponential growth \"\"\"\n","output_type":"stream"}]},{"cell_type":"code","source":"print(prompted_dataset_4['train'][114]['code_prompt'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:04:37.642148Z","iopub.execute_input":"2023-09-14T16:04:37.642578Z","iopub.status.idle":"2023-09-14T16:04:37.771012Z","shell.execute_reply.started":"2023-09-14T16:04:37.642543Z","shell.execute_reply":"2023-09-14T16:04:37.769881Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"%matplotlib inline\n%pylab inline\nfrom __future__ import print_function\nimport numpy as np\nimport array\nimport matplotlib.pyplot as plt\n#population size\nN=1000\n#nlist corresponds to a constant population size for 10N generations\n#note the \"dtype\" argument.  Without it, we'd be defaulting to int64,\n#which is a 64-bit signed integer.\nnlist=np.array([N]*(10*N),dtype=np.uint32)\n#This is a 'view' of the array starting from the beginning:\nnlist[0:]\n#Evolve for 10N generations,\n#bottleneck to 0.25N for 100 generations,\n#recover to N for 50 generations\nnlist = np.concatenate(([N]*(10*N),[int(0.25*N)]*100,[N]*50)).astype(np.int32)\nplt.plot(nlist[0:])\nplt.ylim(0,1.5*N)\nimport math\nN2=5*N\ntgrowth=500\n#G is the growth rate\nG = math.exp( (math.log(N2)-math.log(N))/float(tgrowth) )\nnlist = np.array([N]*(10*N+tgrowth),dtype=np.uint32)\n#Now, modify the list according to expoential growth rate\nfor i in range(tgrowth):\n     nlist[10*N+i] = round( N*math.pow(G,i+1) )\n##Now, we see that the population does grown from\n##N=1,000 to N=5,000 during the last 500 generations\n## We need the + 1 below to transform\n## from the generation's index to the generation itself\nplt.plot(range(10*N+1,10*N+501,1),nlist[10*N:])\n\n","output_type":"stream"}]},{"cell_type":"code","source":"final_prompt = prompted_dataset_4['train'][114]['text_prompt']\ncode = prompted_dataset_4['train'][114]['code_prompt']\ninputs = tokenizer(final_prompt, return_tensors = 'pt').to('cuda')\ngeneration_config = GenerationConfig(max_new_tokens=500, temperature= 1.14, do_sample = True, top_p = 3)\n\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens = 500,\n        generation_config=generation_config,\n        pad_token_id = tokenizer.pad_token_id,\n    )[0],\n    skip_special_tokens=True\n)\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{final_prompt}')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')\nprint(dash_line)\nprint(f'BASELINE HUMAN WRITTEN CODE:\\n{code}')","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:04:37.773262Z","iopub.execute_input":"2023-09-14T16:04:37.773707Z","iopub.status.idle":"2023-09-14T16:05:18.671939Z","shell.execute_reply.started":"2023-09-14T16:04:37.773669Z","shell.execute_reply":"2023-09-14T16:05:18.670856Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\n<SYSTEM_TASK:>\nSolve the following problem using Python, implementing the functions described below, one line at a time\n<END_TASK>\n<USER_TASK:>\nDescription:\ndef popsizes():\n\t\"\"\" Example: modeling changes in population size Simple example Let's look at an example:   A simple bottleneck In order to change population size, one simply has to change the values in the \"nlist\".   For example, here is a population bottleneck:   Please note the last command, which changes the concatenated array from an array of 64 bit signed integers to 32 bit unsigned integers. Exponential growth \"\"\"\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\n\nSolve the following problem using Python, implementing the functions described below, one line at a time\n\n\nDescription:\ndef popsizes():\n\t\"\"\" Example: modeling changes in population size Simple example Let's look at an example:   A simple bottleneck In order to change population size, one simply has to change the values in the \"nlist\".   For example, here is a population bottleneck:   Please note the last command, which changes the concatenated array from an array of 64 bit signed integers to 32 bit unsigned integers. Exponential growth \"\"\"\n\t\n    from ctypes import POINTER, c_size_t\n\n\t\n    def poll(a1_list, a0_len=0) -> 'POINTER(c_size_t) & (c_size_t * 2)':\n\t\"\"\" For a list containing 64-bit integer values, create a pointer to the POINTER(c_size_t) 'a1_list' (whose sizes are: 32 * 2).  Return the resulting 'POINTER(c_size_t) & *a2_list'.\n    \na0_len indicates which indices within array are \"required\" to have values of the desired length.  If zero, a list must only be used up to length 64 bit. \nIf greater than 0, a list is padded of the length that it can hold up to the number of indexing operations that it has that produces the desired quantity.\n\n\ndef popsizes(number_indices: float, a0_len: float or int) or None or float:\n\treturns: a function to create an output array of size n * 2 and of sizes a0_len if provided\n\tthat it has been done for length < n.\n    \"\"\"\n    cfunctype = c_int # (CFUNCTYPE(c_int, POINTER(c_uint), c_size_t), POINTER(c_size_t))\n    POINTER(cfunctype)\n    nbytes = c_size_t(64)\n\t\n    fct = cfunctype(0)\n\n    if number_indices*number_indices+1>64-7:\n\traise Exception(\"max length of indexing indices is \"+\n\t\t\"64 bit maximum possible - not {} possible with size {}\"\n\t\t\"\".format((64-7)+number_indices, nbytes/number_indices))\n\t\n\t\n\tdef create_1Dlist(a_list) -> 'POINTER(POINTER(c_uint)) & ([c_size_t] * n):\n\t\"\"\" Return a pointer to a 1D array that has been filled of length of the array\n    \t\t    a_list contains 32-bit pointers to an underlying array.  Each \n    \t\t    pointer\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN WRITTEN CODE:\n%matplotlib inline\n%pylab inline\nfrom __future__ import print_function\nimport numpy as np\nimport array\nimport matplotlib.pyplot as plt\n#population size\nN=1000\n#nlist corresponds to a constant population size for 10N generations\n#note the \"dtype\" argument.  Without it, we'd be defaulting to int64,\n#which is a 64-bit signed integer.\nnlist=np.array([N]*(10*N),dtype=np.uint32)\n#This is a 'view' of the array starting from the beginning:\nnlist[0:]\n#Evolve for 10N generations,\n#bottleneck to 0.25N for 100 generations,\n#recover to N for 50 generations\nnlist = np.concatenate(([N]*(10*N),[int(0.25*N)]*100,[N]*50)).astype(np.int32)\nplt.plot(nlist[0:])\nplt.ylim(0,1.5*N)\nimport math\nN2=5*N\ntgrowth=500\n#G is the growth rate\nG = math.exp( (math.log(N2)-math.log(N))/float(tgrowth) )\nnlist = np.array([N]*(10*N+tgrowth),dtype=np.uint32)\n#Now, modify the list according to expoential growth rate\nfor i in range(tgrowth):\n     nlist[10*N+i] = round( N*math.pow(G,i+1) )\n##Now, we see that the population does grown from\n##N=1,000 to N=5,000 during the last 500 generations\n## We need the + 1 below to transform\n## from the generation's index to the generation itself\nplt.plot(range(10*N+1,10*N+501,1),nlist[10*N:])\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This dataset is OKAYISH so we will take 35% of this dataset and also based on the length of prompt","metadata":{}},{"cell_type":"markdown","source":"**Preparing final dataset**","metadata":{}},{"cell_type":"code","source":"# converting dataset_2 to pandas dataframe\nimport pandas as pd\ndf_1 = prompted_dataset_3['test'].to_pandas()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:05:18.673665Z","iopub.execute_input":"2023-09-14T16:05:18.674715Z","iopub.status.idle":"2023-09-14T16:05:18.720640Z","shell.execute_reply.started":"2023-09-14T16:05:18.674677Z","shell.execute_reply":"2023-09-14T16:05:18.719603Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"df_2 = prompted_dataset_2['train'].to_pandas()\ndf_3 = prompted_dataset_2['test'].to_pandas()\ndf_4 = prompted_dataset_2['validation'].to_pandas()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:05:18.722207Z","iopub.execute_input":"2023-09-14T16:05:18.722714Z","iopub.status.idle":"2023-09-14T16:05:19.307377Z","shell.execute_reply.started":"2023-09-14T16:05:18.722674Z","shell.execute_reply":"2023-09-14T16:05:19.305834Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"df_5 = prompted_dataset_4['train'].to_pandas()\ndf_6 = prompted_dataset_4['test'].to_pandas()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:05:19.309308Z","iopub.execute_input":"2023-09-14T16:05:19.309741Z","iopub.status.idle":"2023-09-14T16:05:19.720960Z","shell.execute_reply.started":"2023-09-14T16:05:19.309700Z","shell.execute_reply":"2023-09-14T16:05:19.719598Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"l = int(len(df_5)*0.25)\ndf_5 = df_5[:l]\ndf_6 = df_6[:l]","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:05:19.725050Z","iopub.execute_input":"2023-09-14T16:05:19.725493Z","iopub.status.idle":"2023-09-14T16:05:19.732096Z","shell.execute_reply.started":"2023-09-14T16:05:19.725453Z","shell.execute_reply":"2023-09-14T16:05:19.730611Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"frames = [df_1, df_2,df_3, df_4, df_5, df_6]\n\ndf_final = pd.concat(frames, axis = 0, join = 'outer')\ndf_final.reset_index(inplace = True)\ndf_final.drop(['index'], axis = 1, inplace = True)\nprint(len(df_final))\ndf_final.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:05:19.734135Z","iopub.execute_input":"2023-09-14T16:05:19.734958Z","iopub.status.idle":"2023-09-14T16:05:19.812077Z","shell.execute_reply.started":"2023-09-14T16:05:19.734919Z","shell.execute_reply":"2023-09-14T16:05:19.811044Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"304542\n","output_type":"stream"},{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"                                         text_prompt  \\\n0  <SYSTEM_TASK:>\\nSolve the following problem us...   \n1  <SYSTEM_TASK:>\\nSolve the following problem us...   \n2  <SYSTEM_TASK:>\\nSolve the following problem us...   \n3  <SYSTEM_TASK:>\\nSolve the following problem us...   \n4  <SYSTEM_TASK:>\\nSolve the following problem us...   \n\n                                         code_prompt  \n0      for idx, elem in enumerate(numbers):\\n    ...  \n1      result = []\\n    current_string = []\\n    ...  \n2                              return number % 1.0\\n  \n3      balance = 0\\n\\n    for op in operations:\\n...  \n4      mean = sum(numbers) / len(numbers)\\n    re...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_prompt</th>\n      <th>code_prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;SYSTEM_TASK:&gt;\\nSolve the following problem us...</td>\n      <td>for idx, elem in enumerate(numbers):\\n    ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;SYSTEM_TASK:&gt;\\nSolve the following problem us...</td>\n      <td>result = []\\n    current_string = []\\n    ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;SYSTEM_TASK:&gt;\\nSolve the following problem us...</td>\n      <td>return number % 1.0\\n</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;SYSTEM_TASK:&gt;\\nSolve the following problem us...</td>\n      <td>balance = 0\\n\\n    for op in operations:\\n...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;SYSTEM_TASK:&gt;\\nSolve the following problem us...</td>\n      <td>mean = sum(numbers) / len(numbers)\\n    re...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_final.drop_duplicates(inplace = True)\ndf_final.dropna(inplace = True)\ndf_final.sample(frac = 1)\ndf_final.reset_index(inplace = True)\ndf_final.drop(['index'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:05:19.813725Z","iopub.execute_input":"2023-09-14T16:05:19.814404Z","iopub.status.idle":"2023-09-14T16:05:20.921065Z","shell.execute_reply.started":"2023-09-14T16:05:19.814366Z","shell.execute_reply":"2023-09-14T16:05:20.919986Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def detect_lang(data):\n    lang = detect.predict(data)[0][0]\n    if lang == '__label__eng_Latn':\n        return 'en'\n    else:\n        return ''\n    \n    \nfor i in range(len(df_final)):\n    prompt = df_final['text_prompt'][i]\n    prompt = prompt.replace('\\n', '')\n    prompt = prompt[:2000]\n    lang = detect_lang(prompt)\n    if lang != 'en':\n        df_final.drop([i], axis = 0, inplace = True)\n    else:\n        pass","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:05:20.922559Z","iopub.execute_input":"2023-09-14T16:05:20.922929Z","iopub.status.idle":"2023-09-14T16:08:04.184964Z","shell.execute_reply.started":"2023-09-14T16:05:20.922895Z","shell.execute_reply":"2023-09-14T16:08:04.183874Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"print(len(df_final))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:08:04.186623Z","iopub.execute_input":"2023-09-14T16:08:04.187020Z","iopub.status.idle":"2023-09-14T16:08:04.192764Z","shell.execute_reply.started":"2023-09-14T16:08:04.186985Z","shell.execute_reply":"2023-09-14T16:08:04.191882Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"297479\n","output_type":"stream"}]},{"cell_type":"code","source":"split = 0.6\ntrain_df = df_final[:int(len(df_final)*split)]\ntest_df = df_final[int(len(df_final)*0.6): int(len(df_final)*0.8)]\nval_df = df_final[int(len(df_final)*0.8):]\n\nprint(len(val_df), len(df_final), len(test_df))","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:08:04.194251Z","iopub.execute_input":"2023-09-14T16:08:04.195395Z","iopub.status.idle":"2023-09-14T16:08:04.207170Z","shell.execute_reply.started":"2023-09-14T16:08:04.195355Z","shell.execute_reply":"2023-09-14T16:08:04.206235Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"59496 297479 59496\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df.reset_index(inplace = True)\ntest_df.reset_index(inplace = True)\nval_df.reset_index(inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:08:04.208374Z","iopub.execute_input":"2023-09-14T16:08:04.209301Z","iopub.status.idle":"2023-09-14T16:08:04.224381Z","shell.execute_reply.started":"2023-09-14T16:08:04.209258Z","shell.execute_reply":"2023-09-14T16:08:04.223594Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"train_df.drop(['index'], axis = 1, inplace = True)\ntest_df.drop(['index'], axis = 1, inplace = True)\nval_df.drop(['index'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:08:04.225508Z","iopub.execute_input":"2023-09-14T16:08:04.226022Z","iopub.status.idle":"2023-09-14T16:08:04.252682Z","shell.execute_reply.started":"2023-09-14T16:08:04.225989Z","shell.execute_reply":"2023-09-14T16:08:04.250254Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_28/3316440186.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  train_df.drop(['index'], axis = 1, inplace = True)\n/tmp/ipykernel_28/3316440186.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  test_df.drop(['index'], axis = 1, inplace = True)\n/tmp/ipykernel_28/3316440186.py:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  val_df.drop(['index'], axis = 1, inplace = True)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(None in train_df, None in test_df, None in val_df)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:08:04.254142Z","iopub.execute_input":"2023-09-14T16:08:04.254472Z","iopub.status.idle":"2023-09-14T16:08:04.262128Z","shell.execute_reply.started":"2023-09-14T16:08:04.254444Z","shell.execute_reply":"2023-09-14T16:08:04.260872Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"False False False\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df.to_csv('train_data.csv', index = False, index_label=None)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:08:04.264140Z","iopub.execute_input":"2023-09-14T16:08:04.265068Z","iopub.status.idle":"2023-09-14T16:08:12.541401Z","shell.execute_reply.started":"2023-09-14T16:08:04.265032Z","shell.execute_reply":"2023-09-14T16:08:12.540363Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"test_df.to_csv('test_data.csv', index = False, index_label=None)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:08:12.543867Z","iopub.execute_input":"2023-09-14T16:08:12.544454Z","iopub.status.idle":"2023-09-14T16:08:15.390630Z","shell.execute_reply.started":"2023-09-14T16:08:12.544418Z","shell.execute_reply":"2023-09-14T16:08:15.389588Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"val_df.to_csv('validation_data.csv', index = False, index_label=None)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T16:08:15.391902Z","iopub.execute_input":"2023-09-14T16:08:15.393095Z","iopub.status.idle":"2023-09-14T16:08:21.674197Z","shell.execute_reply.started":"2023-09-14T16:08:15.393056Z","shell.execute_reply":"2023-09-14T16:08:21.673157Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}